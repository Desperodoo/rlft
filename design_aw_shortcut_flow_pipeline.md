# AW-ShortCut Flow Pipeline: ä»Offline BCåˆ°Online RLçš„å®Œæ•´ä¸‰é˜¶æ®µè®¾è®¡

## ğŸ¯ æ ¸å¿ƒç›®æ ‡ä¸æ€è·¯

### ä¸ºä»€ä¹ˆéœ€è¦ä¸­é—´å±‚ï¼Ÿ

ç›´æ¥ä» ShortCut Flow BC è·³åˆ° ReinFlow Online RL ä¼šé¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼š

1. **Critic æœªæ ¡å‡†**ï¼šBC é˜¶æ®µæ²¡æœ‰ criticï¼Œçªç„¶åŠ å…¥ RL æ—¶ï¼Œcritic çš„å€¼å‡½æ•°ä¼°è®¡å¯èƒ½ä¸å¯é 
2. **Policy æœªå‡†å¤‡**ï¼šå¦‚æœç›´æ¥ç”¨ reward é©±åŠ¨ ShortCut çš„ step sizeï¼Œå¯èƒ½å¯¼è‡´ OOD exploration + critic çˆ†ç‚¸

### ä¸­é—´ Offline RL å±‚çš„ä½œç”¨

**ä¸æ˜¯**è¦æŠŠ policy å˜æˆ"ç¦»çº¿æœ€ä¼˜"ï¼Œè€Œæ˜¯ï¼š

- âœ… è®© policy åœ¨ demo åˆ†å¸ƒé™„è¿‘**ç¨å¾®æœé«˜Qæ ·æœ¬é æ‹¢**ï¼ˆä½†ä¸è·‘å‡ºåˆ†å¸ƒï¼‰
- âœ… æŠŠ critic åˆæ­¥**æ ¡å‡†**åˆ°æ•°æ®åˆ†å¸ƒï¼Œé˜²æ­¢åç»­çˆ†ç‚¸
- âœ… ä¿æŒ **ODE/shortcut çš„å‡ ä½•ç»“æ„å®Œæ•´**ï¼Œä¸º online RL é˜¶æ®µåšå¥½é“ºå«

### ä¸ºä»€ä¹ˆé€‰æ‹© Advantage-Weighted è€Œéç›´æ¥ Q-Learningï¼Ÿ

ä½ çš„ AWCP å®éªŒå·²ç»è¯æ˜äº†ï¼š

| æ–¹æ³• | Critic ç¨³å®šæ€§ | Success Rate | é—®é¢˜ |
|------|-------------|--------------|------|
| **ç›´æ¥ Q æœ€å¤§åŒ–** | âŒ æ˜“çˆ†ç‚¸ | ä½ | Policy è·‘åˆ° OOD åŒºåŸŸï¼ŒCritic æ— æ•°æ® |
| **Advantage-Weighted BC** | âœ… ç¨³å®š | é«˜ | Policy stay in distributionï¼ŒCritic åªå­¦æ•°æ®åŒºåŸŸ |

å› æ­¤ä¸­é—´å±‚ä¸€å®šè¦ç”¨ **IQL/AWAC é£æ ¼çš„åŠ æƒ BC**ï¼Œè€Œä¸æ˜¯ CPQL å¼çš„ç›´æ¥ Q maximizationã€‚

---

## 1ï¸âƒ£ AW-ShortCut Flow (AW-SCF) çš„è“å›¾

### 1.1 æ ¸å¿ƒæ¶æ„

```
è¾“å…¥ï¼šä¸“å®¶æ¼”ç¤ºè½¨è¿¹ (s, a_sequence)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Policy Network: ShortCutFlowAgent    â”‚
â”‚  - velocity_net (with step size d)   â”‚
â”‚  - EMA velocity_net (for consistency)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ sample actions
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Critic Network: DoubleQNetwork       â”‚
â”‚  - Q1, Q2 for conservative estimate  â”‚
â”‚  - Target Critic (EMA update)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ compute advantage
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Loss Combination:                    â”‚
â”‚  1. AW-Flow Loss (Q-weighted)        â”‚
â”‚  2. AW-Shortcut Loss (Q-weighted)    â”‚
â”‚  3. Critic Loss (SMDP Bellman)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Policy ç½‘ç»œï¼šç»§æ‰¿ ShortCutFlowAgent

**ä¸éœ€è¦æ”¹åŠ¨ ShortCutFlowAgent çš„ç»“æ„**ï¼Œåªæ˜¯åœ¨ `compute_loss()` ä¸­åŠ å…¥ Q-weightingï¼š

```python
class AWShortCutFlowAgent(nn.Module):
    def __init__(
        self,
        velocity_net: ShortCutVelocityUNet1D,
        critic: DoubleQNetwork,
        action_dim: int,
        obs_horizon: int = 2,
        pred_horizon: int = 16,
        act_horizon: int = 8,
        # ShortCut parameters
        max_denoising_steps: int = 8,
        step_size_mode: str = "fixed",      # æ¨èç”¨ fixed æˆ– uniform
        fixed_step_size: float = 0.0625,    # 1/16
        target_mode: str = "velocity",       # å¿…é¡»ç”¨ velocity
        teacher_steps: int = 1,              # å°æ­¥ local approximation
        use_ema_teacher: bool = True,
        # Offline RL parameters
        beta: float = 0.5,                   # Advantage weighting
        weight_clip: float = 10.0,           # Prevent outliers
        bc_weight: float = 1.0,              # Flow matching weight
        consistency_weight: float = 0.3,     # Shortcut weight (light)
        reward_scale: float = 0.1,
        q_target_clip: float = 100.0,
        tau: float = 0.005,                  # Soft update for target critic
        gamma: float = 0.99,
        ema_decay: float = 0.999,
        device: str = "cuda",
    ):
        super().__init__()
        # ShortCut Flow components
        self.velocity_net = velocity_net
        self.velocity_net_ema = copy.deepcopy(velocity_net)
        for p in self.velocity_net_ema.parameters():
            p.requires_grad = False
        
        # Critic components
        self.critic = critic
        self.critic_target = copy.deepcopy(critic)
        for p in self.critic_target.parameters():
            p.requires_grad = False
        
        # Offline RL hyperparameters
        self.beta = beta
        self.weight_clip = weight_clip
        self.bc_weight = bc_weight
        self.consistency_weight = consistency_weight
        self.reward_scale = reward_scale
        self.q_target_clip = q_target_clip
        self.tau = tau
        self.gamma = gamma
        self.ema_decay = ema_decay
        
        # Store ShortCut parameters for later use
        self.action_dim = action_dim
        self.pred_horizon = pred_horizon
        self.act_horizon = act_horizon
        self.step_size_mode = step_size_mode
        self.fixed_step_size = fixed_step_size
        self.target_mode = target_mode
        self.teacher_steps = teacher_steps
```

### 1.3 å…³é”® Loss å‡½æ•°è®¾è®¡

#### Loss 1: Advantage-Weighted Flow Loss

```python
def _compute_flow_loss(self, obs_cond, actions, actions_for_q):
    """Flow matching loss with Q-based weighting."""
    
    # Step 1: è®¡ç®— advantage-based æƒé‡ï¼ˆç›´æ¥å€Ÿé‰´ AWCPï¼‰
    with torch.no_grad():
        q1, q2 = self.critic(actions_for_q, obs_cond)
        q_data = torch.min(q1, q2)
        baseline = q_data.mean()
        advantage = q_data - baseline
        
        # AWAC-style exponential weighting
        weights = torch.clamp(
            torch.exp(self.beta * advantage),
            max=self.weight_clip
        )
        weights = weights / weights.mean()  # Normalize
        weights = weights.squeeze(-1)
    
    # Step 2: ShortCut Flow çš„æ ‡å‡†é‡‡æ ·ï¼ˆæ¥è‡ª ShortCutFlowAgentï¼‰
    B = actions.shape[0]
    device = actions.device
    
    # Sample noise and time
    x_0 = torch.randn_like(actions)
    t = torch.rand(B, device=device)
    
    # Interpolate
    t_expand = t.view(-1, 1, 1)
    x_t = (1 - t_expand) * x_0 + t_expand * actions
    
    # Target velocity
    v_target = actions - x_0
    
    # Sample step size d (æ¥è‡ª ShortCutFlowAgent._sample_step_size)
    d = self._sample_step_size_fixed(B, device)  # æ¨èç”¨ fixed æˆ– small uniform
    
    # Predict velocity with step size d
    v_pred = self.velocity_net(x_t, t, d, obs_cond)
    
    # Step 3: è®¡ç®— per-sample lossï¼Œç„¶åç”¨æƒé‡åŠ æƒ
    flow_loss_per_sample = F.mse_loss(v_pred, v_target, reduction="none")
    flow_loss_per_sample = flow_loss_per_sample.mean(dim=(1, 2))  # [B]
    
    # Weighted average
    flow_loss = (weights * flow_loss_per_sample).mean()
    
    return flow_loss, weights
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- é«˜ Q æ ·æœ¬çš„ flow loss è·å¾—æ›´é«˜æƒé‡
- ä½ Q æ ·æœ¬çš„ flow loss è·å¾—æ›´ä½æƒé‡
- Policy ä¸è·‘å‡º demo åˆ†å¸ƒï¼ˆå› ä¸ºä»ç„¶åœ¨æœ€å°åŒ–ä¸æ‰€æœ‰ demo çš„è·ç¦»ï¼‰

#### Loss 2: Advantage-Weighted Shortcut Consistency Loss

```python
def _compute_shortcut_loss(self, obs_cond, actions, weights):
    """Shortcut consistency loss with Q-based weighting."""
    
    B = actions.shape[0]
    device = actions.device
    
    # ä» ShortCutFlowAgent ä¸­æŠ å‡ºæ¥çš„é€»è¾‘
    x_0 = torch.randn_like(actions)
    
    # Sample time and step size for consistency
    t = self.t_min + torch.rand(B, device=device) * (self.t_max - self.t_min)
    delta_t = 0.02 + torch.rand(B, device=device) * (0.15 - 0.02)
    t_plus = torch.clamp(t + delta_t, max=self.t_max)
    
    # Interpolate
    t_expand = t.view(-1, 1, 1)
    t_plus_expand = t_plus.view(-1, 1, 1)
    x_t = (1 - t_expand) * x_0 + t_expand * actions
    x_t_plus = (1 - t_plus_expand) * x_0 + t_plus_expand * actions
    
    # Teacher target (from EMA network)
    with torch.no_grad():
        x_teacher = x_t_plus.clone()
        current_t = t_plus.clone()
        remaining_time = 1.0 - current_t
        dt_teacher = remaining_time / self.teacher_steps
        
        for _ in range(self.teacher_steps):
            v_teacher = self.velocity_net_ema(
                x_teacher, current_t, 
                self.fixed_step_size,  # æˆ–ä» step size head é‡‡æ ·
                obs_cond
            )
            dt_expand = dt_teacher.view(-1, 1, 1)
            x_teacher = x_teacher + v_teacher * dt_expand
            current_t = current_t + dt_teacher
        
        target = x_teacher if self.target_mode == "endpoint" else (x_teacher - x_0)
    
    # Student consistency prediction
    if self.target_mode == "endpoint":
        v_pred = self.velocity_net(x_t_plus, t_plus, self.fixed_step_size, obs_cond)
        # è¿™é‡Œå®é™…ä¸Šåº”è¯¥æ˜¯ç”¨ 2*d çš„æ­¥é•¿é¢„æµ‹ï¼Œä½†ä¸ºäº†ä¸ offline é˜¶æ®µçš„ä¿å®ˆæ€§å¯¹é½ï¼Œ
        # å…ˆç”¨å›ºå®šå°æ­¥
        consistency_loss_per_sample = F.mse_loss(
            x_t_plus + v_pred * 0.0625,  # ä¸€æ­¥å°æ­¥
            target,
            reduction="none"
        ).mean(dim=(1, 2))
    else:  # velocity mode
        v_target = target
        v_pred = self.velocity_net(x_t_plus, t_plus, self.fixed_step_size, obs_cond)
        consistency_loss_per_sample = F.mse_loss(
            v_pred, v_target, reduction="none"
        ).mean(dim=(1, 2))
    
    # å…³é”®ï¼šç”¨åŒæ ·çš„æƒé‡ä¹˜
    consistency_loss = (weights * consistency_loss_per_sample).mean()
    
    return consistency_loss
```

**å…³é”®**ï¼šshortcut loss ä¹Ÿç”¨åŒæ ·çš„æƒé‡ä¹˜ï¼Œè¿™æ ·ç¡®ä¿é«˜ Q æ ·æœ¬çš„"å­¦ä¼šå¤§æ­¥"ä¹Ÿè¢«å¼ºåŒ–ï¼Œä½ Q æ ·æœ¬è¢«å»æƒé‡åŒ–ã€‚

#### Loss 3: Critic Lossï¼ˆSMDP Bellmanï¼Œç›´æ¥ç”¨ AWCP çš„é€»è¾‘ï¼‰

```python
def _compute_critic_loss(self, obs_cond, next_obs_cond, actions_for_q, 
                         rewards, dones, cumulative_reward=None, 
                         chunk_done=None, discount_factor=None):
    """Critic loss using SMDP Bellman equation."""
    
    # Use SMDP fields if provided
    if cumulative_reward is not None:
        r = cumulative_reward
        d = chunk_done if chunk_done is not None else dones
        gamma_tau = discount_factor if discount_factor is not None else torch.full_like(r, self.gamma)
    else:
        r = rewards
        d = dones
        gamma_tau = torch.full_like(r if r.dim() == 1 else r.squeeze(-1), self.gamma)
    
    # Ensure proper shape
    if r.dim() == 1:
        r = r.unsqueeze(-1)
    if d.dim() == 1:
        d = d.unsqueeze(-1)
    if gamma_tau.dim() == 1:
        gamma_tau = gamma_tau.unsqueeze(-1)
    
    # Scale rewards
    scaled_rewards = r * self.reward_scale
    
    with torch.no_grad():
        # Sample next actions using EMA policy
        next_actions_full = self._sample_actions_batch(next_obs_cond, use_ema=True)
        next_actions = next_actions_full[:, :self.act_horizon, :]
        
        # Compute target Q-values (conservative double Q)
        target_q1, target_q2 = self.critic_target(next_actions, next_obs_cond)
        target_q = torch.min(target_q1, target_q2)
        
        # TD target
        target_q = scaled_rewards + (1 - d) * gamma_tau * target_q
        
        if self.q_target_clip is not None:
            target_q = torch.clamp(target_q, -self.q_target_clip, self.q_target_clip)
    
    # Current Q-values
    current_q1, current_q2 = self.critic(actions_for_q, obs_cond)
    
    # MSE loss
    critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)
    
    return critic_loss
```

### 1.4 å®Œæ•´çš„ `compute_loss()` æ–¹æ³•

```python
def compute_loss(self, obs_features, actions, rewards, next_obs_features, dones,
                 actions_for_q=None, cumulative_reward=None, 
                 chunk_done=None, discount_factor=None):
    """Compute AW-ShortCut Flow loss: AW-Flow + AW-Shortcut + Critic."""
    
    if actions_for_q is None:
        actions_for_q = actions
    
    # Flatten obs_features if needed
    if obs_features.dim() == 3:
        obs_cond = obs_features.reshape(obs_features.shape[0], -1)
    else:
        obs_cond = obs_features
    
    if next_obs_features.dim() == 3:
        next_obs_cond = next_obs_features.reshape(next_obs_features.shape[0], -1)
    else:
        next_obs_cond = next_obs_features
    
    # Compute AW-Flow loss (includes weights)
    flow_loss, weights = self._compute_flow_loss(obs_cond, actions, actions_for_q)
    
    # Compute AW-Shortcut loss (uses same weights)
    shortcut_loss = self._compute_shortcut_loss(obs_cond, actions, weights)
    
    # Compute Critic loss
    critic_loss = self._compute_critic_loss(
        obs_cond, next_obs_cond, actions_for_q, rewards, dones,
        cumulative_reward=cumulative_reward,
        chunk_done=chunk_done,
        discount_factor=discount_factor,
    )
    
    # Total loss
    policy_loss = self.bc_weight * flow_loss + self.consistency_weight * shortcut_loss
    total_loss = policy_loss + critic_loss
    
    return {
        "loss": total_loss,
        "policy_loss": policy_loss,
        "flow_loss": flow_loss,
        "shortcut_loss": shortcut_loss,
        "critic_loss": critic_loss,
        "weight_mean": weights.mean(),
        "weight_std": weights.std(),
    }
```

---

## 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¿™ä¸ªè®¾è®¡èƒ½è§£å†³ CPQL çš„é—®é¢˜ï¼Ÿ

### CPQL å¤±è´¥çš„åŸå› å›é¡¾

ä½ ä¹‹å‰å®ç° CPQL æ—¶å‘ç°ï¼š

```
ç›´æ¥åš Q-learning æœ€å¤§åŒ–:
  policy_loss = bc_loss + alpha * (-Q(s, Ï€(s)))
  â†“
  Policy å¿«é€Ÿç§»å‘ OOD åŠ¨ä½œåŒºåŸŸ
  â†“
  Critic åœ¨é‚£äº›åŒºåŸŸæ²¡è§è¿‡æ•°æ®ï¼Œåªèƒ½ bootstrap èƒ¡çŒœ
  â†“
  Q å€¼çˆ†ç‚¸ + Policy å­¦çš„æ˜¯ critic çš„å¹»è§‰
```

### AW-ShortCut Flow å¦‚ä½•é¿å…

```
Advantage-Weighted BC:
  w_i = exp(Î² * (Q_i - baseline))
  policy_loss = Î£ w_i * (flow_loss_i + consistency_loss_i)
  â†“
  Policy å§‹ç»ˆåœ¨ demo è½¨è¿¹çº¿ä¸ŠåšåŠ æƒ BC
  â†“
  ä¸ä¼šè·‘åˆ° OOD åŒºåŸŸ
  â†“
  Critic åªéœ€åœ¨æ•°æ®åˆ†å¸ƒä¸Š self-consistentï¼ˆç”¨ SMDP Bellmanï¼‰
  â†“
  æ›´ç¨³å®šï¼ŒQ ä¸çˆ†ç‚¸
```

### ç†è®ºæ”¯æŒ

è¿™ä¸ªæ€æƒ³æ¥è‡ª **IQL (Implicit Q-Learning)** å’Œ **AWAC**ï¼š

| ç‰¹æ€§ | CPQL-style | AWAC/IQL-style |
|------|-----------|-----------------|
| Policy çº¦æŸ | âŒ æ— çº¦æŸï¼Œæœ€å¤§åŒ–Q | âœ… é™åˆ¶åœ¨æ•°æ®åˆ†å¸ƒï¼Œç”¨QåŠ æƒ |
| Q çš„ä½œç”¨ | âŒ é©±åŠ¨ policy ç¦»å¼€åˆ†å¸ƒ | âœ… åŒºåˆ†"å¥½æ ·æœ¬"vs"å·®æ ·æœ¬" |
| Critic å­¦ä¹  | âŒ éœ€è¦ generalize åˆ° OOD | âœ… åªéœ€åœ¨æ•°æ®åŒºåŸŸ self-consistent |
| ç¨³å®šæ€§ | âŒ æ˜“çˆ†ç‚¸ | âœ… ä¿å®ˆä¸”ç¨³å®š |
| é€‚åˆåœºæ™¯ | âŒ åœ¨çº¿RL | âœ… ç¦»çº¿RL + åˆ°åœ¨çº¿çš„è¿‡æ¸¡ |

---

## 3ï¸âƒ£ å®Œæ•´çš„ä¸‰é˜¶æ®µç®¡é“è®¾è®¡

### Stage 1: ShortCut Flowé¢„è®­ç»ƒï¼ˆçº¯ BCï¼Œ~100k stepsï¼‰

**é…ç½®**ï¼ˆåŸºäº sweep ç»“æœçš„æ¨èï¼‰ï¼š

```yaml
# Model
algorithm: "shortcut_flow"

# ShortCut Flow parameters (from sweep best practices)
sc_target_mode: "velocity"                    # âœ“ å­¦ local solver
sc_use_ema_teacher: true                      # âœ“ EMA ç¨³å®š
sc_teacher_steps: 1                           # âœ“ ä¿ç•™å±€éƒ¨æ€§
sc_step_size_mode: "fixed"                    # âœ“ å¯é æ€§ä¼˜å…ˆ
sc_fixed_step_size: 0.0625                    # âœ“ 1/16 å°æ­¥
sc_t_sampling_mode: "uniform"                 # âœ“ å…¨è¦†ç›–æ— å
sc_inference_mode: "uniform"                  # âœ“ åˆ†å¸ƒåŒ¹é…
sc_num_inference_steps: 8

# Loss weights
flow_weight: 1.0
consistency_weight: 0.3                       # âœ“ shortcut è½»é‡
self_consistency_k: 0.1                       # âœ“ ä½æ¯”ä¾‹é‡‡æ ·

# Training
total_iters: 100000
eval_freq: 2000
batch_size: 128
learning_rate: 1e-4
```

**ç›®æ ‡**ï¼š
- âœ… å­¦å¥½ local ODE solverï¼ˆvelocity ç²¾ç¡®ï¼‰
- âœ… shortcut èƒ½åŠ›æ¸©å’Œä½†å¯é 
- âœ… ä¸º offline RL åšå¥½åŸºç¡€

**ç›‘æ§æŒ‡æ ‡**ï¼š
- `flow_loss`: åº”è¯¥å•è°ƒä¸‹é™
- `shortcut_loss`: å¯èƒ½æŒ¯è¡ï¼Œä½†æ•´ä½“è¶‹åŠ¿æ˜¯ä¸‹é™
- `success_once`: æœ€ç»ˆ > 0.40

### Stage 2: AW-ShortCut Flow ç¦»çº¿ RLï¼ˆ~20k-50k stepsï¼‰

**é…ç½®**ï¼ˆåŸºäºæ­¤æ–‡æ¡£çš„è®¾è®¡ï¼‰ï¼š

```yaml
# Model
algorithm: "aw_shortcut_flow"

# ç»§æ‰¿ Stage 1 çš„ ShortCut å‚æ•°ï¼ˆfreeze æˆ–è½»å¾®è°ƒæ•´ï¼‰
sc_target_mode: "velocity"
sc_use_ema_teacher: true
sc_teacher_steps: 1
sc_step_size_mode: "fixed"
sc_fixed_step_size: 0.0625
sc_t_sampling_mode: "uniform"
sc_inference_mode: "uniform"                  # ä¿æŒåˆ†å¸ƒåŒ¹é…
sc_num_inference_steps: 8

# Offline RL parameters (æ–°å¢)
beta: 0.5                                     # Advantage weighting temperature
weight_clip: 10.0                             # Prevent outliers
reward_scale: 0.1                             # Scale rewards
q_target_clip: 100.0                          # Clip critic targets
tau: 0.005                                    # Soft update for target critic
gamma: 0.99                                   # Discount factor

# Loss weights (ä¸ Stage 1 ç›¸æ¯”ï¼šflow ä»ä¸»å¯¼)
flow_weight: 1.0
consistency_weight: 0.3
bc_weight: 1.0                                # æ–°å¢ï¼šä¿è¯ policy ä¸æ¼‚ç¦»åˆ†å¸ƒ

# Training
total_iters: 50000
eval_freq: 1000
batch_size: 128
learning_rate: 1e-4                           # å¯èƒ½ç•¥é™ (1e-5 ~ 1e-4)
warmup_iters: 2000                            # å‰ 2k steps ç”¨çº¯ BC warm-up
```

**é‡ç‚¹è¶…å‚**ï¼š

| è¶…å‚ | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `beta` | 0.5 ~ 1.0 | 0.5 è¾ƒä¿å®ˆï¼Œ1.0 è¾ƒæ¿€è¿›ã€‚å¦‚æœ critic ä¸ç¨³å®šï¼Œç”¨ 0.5 |
| `weight_clip` | 5.0 ~ 10.0 | é˜²æ­¢å°‘æ•°é«˜Qæ ·æœ¬ä¸»å¯¼ã€‚10.0 è¾ƒå®½æ¾ |
| `reward_scale` | 0.05 ~ 0.2 | è¾ƒå°çš„å€¼ä¿è¯ Q ä¸è¦é•¿å¾—å¤ªå¿« |
| `bc_weight` | 1.0 å›ºå®š | è¿™ä¸ªä¸åŠ¨ï¼Œä¿è¯ BC loss å§‹ç»ˆä¸»å¯¼ |
| `consistency_weight` | 0.3 å›ºå®š | ç»§æ‰¿ Stage 1 |

**ç›®æ ‡**ï¼š
- âœ… Critic åœ¨æ•°æ®åˆ†å¸ƒä¸Šæ ¡å‡†ï¼Œloss ç¨³å®šä¸‹é™ï¼ˆä¸çˆ†ç‚¸ï¼‰
- âœ… Policy å¾®å¾®å‘é«˜Qæ ·æœ¬é æ‹¢ï¼Œsuccess æœ‰ 5-10% æå‡
- âœ… ä¿æŒ ODE/shortcut ç»“æ„å®Œæ•´
- âœ… ä¸º online RL æä¾› warm-start policy + critic

**ç›‘æ§æŒ‡æ ‡**ï¼š
- `critic_loss`: åº”è¯¥å•è°ƒä¸‹é™ï¼Œä¸”ä¸åº”è¯¥è¶…è¿‡åˆå€¼çš„ 100 å€
- `flow_loss + shortcut_loss`: åº”è¯¥ä¿æŒç›¸å¯¹ç¨³å®šï¼ˆPolicy ä¸å‘æ•£ï¼‰
- `weight_mean / weight_std`: æƒé‡åº”è¯¥åˆç†åˆ†å¸ƒï¼ˆä¸åº”è¯¥æŸä¸ªæ ·æœ¬æƒé‡ >>1ï¼‰
- `success_once`: åº”è¯¥æœ‰ 5-10% çš„æå‡ï¼ˆe.g., 0.40 â†’ 0.44-0.45ï¼‰

**å¦‚æœå‡ºé—®é¢˜çš„è°ƒè¯•**ï¼š

| é—®é¢˜ | åŸå›  | è°ƒæ•´ |
|------|------|------|
| Critic loss ç‚¸æ‰ | reward_scale å¤ªå¤§ æˆ– Q bootstrap ä¸ç¨³å®š | â†“ reward_scale åˆ° 0.05ï¼Œæˆ–â†“ beta åˆ° 0.3 |
| Success æ²¡æœ‰æå‡ | Advantage weighting è¿‡å¼± æˆ– reward signal æœ¬èº«é—®é¢˜ | â†‘ beta åˆ° 1.0ï¼Œæˆ–æ£€æŸ¥ reward è®¡ç®— |
| Policy loss å¤§å¹…æŒ¯è¡ | æƒé‡åˆ†å¸ƒä¸å‡ | â†“ weight_clipï¼Œæˆ–â†‘ warmup_iters |
| Actor-Critic diverge | Policy å’Œ Critic æ›´æ–°ä¸åŒæ­¥ | â†“ learning_rateï¼Œæˆ–äº¤æ›¿æ›´æ–° |

### Stage 3: ReinFlow åœ¨çº¿ RLï¼ˆä» Stage 2 åˆå§‹åŒ–ï¼Œ~100k stepsï¼‰

**é…ç½®**ï¼ˆä»¥ Stage 2 çš„ checkpoint åˆå§‹åŒ–ï¼‰ï¼š

```yaml
# Model
algorithm: "reinflow"

# ä» Stage 2 åŠ è½½ checkpoint
pretrained_velocity_net: "stage2_checkpoint.pt"
pretrained_critic: "stage2_critic.pt"

# ReinFlow ç‰¹å®šå‚æ•°
num_flow_steps: 8
ema_decay: 0.999
gamma: 0.99
gae_lambda: 0.95
clip_ratio: 0.2
entropy_coef: 0.01
value_coef: 0.5

# åœ¨çº¿é˜¶æ®µçš„ shortcut é…ç½®ï¼ˆé€æ¸æ”¾æ¾ï¼‰
sc_inference_mode: "uniform"                  # åˆæœŸä»ç”¨ uniform
sc_num_inference_steps: 8

# On-policy é‡‡æ ·
num_envs: 10
num_steps_per_env: 200                        # Rollout length
num_epochs: 4                                 # PPO epochs
num_minibatches: 4

# Training
total_iters: 100000
eval_freq: 1000
```

**ä¸‰å­é˜¶æ®µç­–ç•¥**ï¼ˆå¯é€‰ curriculumï¼‰ï¼š

1. **é˜¶æ®µ 3a** (~25k steps)ï¼šå†»ç»“ criticï¼Œåªç”¨ BC loss + PPO policy loss
   - ç›®çš„ï¼šè®© policy è½»å¾®é€‚åº” online ç¯å¢ƒ

2. **é˜¶æ®µ 3b** (~50k steps)ï¼šè§£å†» criticï¼Œå¼•å…¥ Q-learning
   - ç›®çš„ï¼šCritic å­¦ä¹ åœ¨çº¿è½¨è¿¹çš„ä»·å€¼

3. **é˜¶æ®µ 3c** (~25k steps)ï¼šé€æ­¥å¼€å¯ adaptive inference
   - ç›®çš„ï¼šè®© shortcut å­¦ä¼šåœ¨é«˜ reward åŒºåŸŸè·³å¤§æ­¥

**ç›‘æ§æŒ‡æ ‡**ï¼š
- æ ‡å‡† PPO æŒ‡æ ‡ï¼špolicy_loss, value_loss, entropy
- Critic æŒ‡æ ‡ï¼šcritic_loss, target_q stdï¼ˆä¸åº”è¯¥çˆ†ç‚¸ï¼‰
- Exploration æŒ‡æ ‡ï¼šepisode_reward, success_rateï¼ˆåº”è¯¥å•è°ƒä¸Šå‡ï¼‰

---

## 4ï¸âƒ£ ä»£ç é›†æˆæ£€æŸ¥æ¸…å•

### å¿…éœ€çš„æ–°å¢/ä¿®æ”¹

#### æ–°æ–‡ä»¶ï¼š`algorithms/aw_shortcut_flow.py`

```python
# ä¸»è¦å†…å®¹ï¼šAWShortCutFlowAgent ç±»
# - ç»§æ‰¿ ShortCutFlowAgent çš„ step size / time sampling / shortcut target é€»è¾‘
# - æ–°å¢ criticï¼ˆDoubleQNetworkï¼‰å’Œ target_critic
# - æ–°å¢ _compute_flow_lossã€_compute_shortcut_lossã€_compute_critic_loss
# - å®ç° compute_lossã€update_emaã€update_targetã€get_action ç­‰æ–¹æ³•
```

#### ä¿®æ”¹ï¼š`train_offline_rl.py`

æ·»åŠ  AW-ShortCut Flow çš„å‘½ä»¤è¡Œå‚æ•°ï¼š

```python
# æ—¢æœ‰çš„ ShortCut Flow å‚æ•°ç»§ç»­æ”¯æŒ
parser.add_argument("--sc_target_mode", type=str, default="velocity")
parser.add_argument("--sc_use_ema_teacher", action="store_true", default=True)
# ... å…¶ä»– sc_ å‚æ•°

# æ–°å¢ AW-ShortCut Flow å‚æ•°
parser.add_argument("--beta", type=float, default=0.5, 
                    help="Advantage weighting temperature")
parser.add_argument("--weight_clip", type=float, default=10.0,
                    help="Max weight to prevent outliers")
parser.add_argument("--reward_scale", type=float, default=0.1)
parser.add_argument("--q_target_clip", type=float, default=100.0)

# ä¿®æ”¹ create_agent å‡½æ•°
if algorithm == "aw_shortcut_flow":
    agent = AWShortCutFlowAgent(
        velocity_net=velocity_net,
        critic=critic,
        action_dim=action_dim,
        obs_horizon=obs_horizon,
        pred_horizon=pred_horizon,
        act_horizon=act_horizon,
        beta=args.beta,
        weight_clip=args.weight_clip,
        reward_scale=args.reward_scale,
        q_target_clip=args.q_target_clip,
        # ... å…¶ä»–å‚æ•°
    )
```

#### ä¿®æ”¹ï¼š`algorithms/__init__.py`

```python
from .aw_shortcut_flow import AWShortCutFlowAgent

__all__ = [
    # ...
    "AWShortCutFlowAgent",
    # ...
]
```

### Sweep è„šæœ¬

å¯å‚è€ƒç°æœ‰çš„ `sweep_awcp_beta_parallel.sh` çš„ç»“æ„ï¼Œåˆ›å»º `sweep_aw_scf_offline_rl.sh`ï¼š

```bash
#!/bin/bash
# AW-ShortCut Flow Offline RL è¶…å‚æ‰«æ

BETAS=(0.3 0.5 1.0)
WEIGHT_CLIPS=(5.0 10.0)
REWARD_SCALES=(0.05 0.1 0.2)

for beta in "${BETAS[@]}"; do
    for wc in "${WEIGHT_CLIPS[@]}"; do
        for rs in "${REWARD_SCALES[@]}"; do
            # ä» Stage 1 checkpoint åˆå§‹åŒ–
            python train_offline_rl.py \
                --algorithm aw_shortcut_flow \
                --pretrained_velocity_net "stage1_best.pt" \
                --beta $beta \
                --weight_clip $wc \
                --reward_scale $rs \
                --exp_name "aw_scf_offline_beta${beta}_wc${wc}_rs${rs}" \
                # ... å…¶ä»–å‚æ•°
        done
    done
done
```

---

## 5ï¸âƒ£ ç†è®ºæ€»ç»“

### ä¸ºä»€ä¹ˆè¿™ä¸ªç®¡é“åœ¨ç†è®ºä¸Šæ˜¯åˆç†çš„ï¼Ÿ

#### 1. **Off-policy æ•°æ®çš„ç¨³å®šå­¦ä¹ ** (Offline RL å…±è¯†)

åœ¨ç¦»çº¿è®¾ç½®ä¸­ï¼Œç›´æ¥åš on-policy æ¢¯åº¦ï¼ˆå¦‚ REINFORCE æˆ– PPOï¼‰æ˜¯ä¸å®‰å…¨çš„ï¼Œå› ä¸ºï¼š
- æ–°é‡‡æ ·çš„è½¨è¿¹ä¼šè¿œç¦»æ•°æ®åˆ†å¸ƒ
- Critic æ²¡æœ‰åœ¨é‚£äº›åŒºåŸŸçš„æ•°æ®ï¼Œä¼°è®¡ä¸å¯é 

**Advantage-Weighted BC** é€šè¿‡ä»¥ä¸‹æœºåˆ¶è§£å†³ï¼š
- Policy å§‹ç»ˆåš BCï¼ˆç”Ÿæˆæ¥è¿‘ demo çš„åŠ¨ä½œï¼‰
- ç”¨ Q æ¥é€‰æ‹©æ€§å¼ºåŒ–"æ›´å¥½çš„ demo"
- ç»“æœï¼špolicy åœ¨æ•°æ®åˆ†å¸ƒé™„è¿‘åšå¾®å°è°ƒæ•´ï¼ŒCritic çš„ä¼°è®¡è¯¯å·®æœ‰ç•Œ

#### 2. **ShortCut Flow çš„ Local Approximation æ€§è´¨** (from sweep)

Sweep ç»“è®ºï¼šShortCut Flow æœ¬è´¨ä¸Šåœ¨å­¦"å±€éƒ¨ ODE solver"ï¼Œå› æ­¤ï¼š
- å°æ­¥é•¿ + EMA teacher + velocity target æœ€ä¼˜
- ä¸é€‚åˆæ¿€è¿›çš„å¤§æ­¥æ¢ç´¢ï¼ˆä¼šç ´åå±€éƒ¨ç»“æ„ï¼‰

**AW-ShortCut Flow** çš„ä¼˜åŠ¿ï¼š
- Policy ä¿æŒåœ¨ demo åˆ†å¸ƒï¼Œä¸ä¼šå­¦å¥‡æ€ªçš„å¤§æ­¥
- Shortcut èƒ½åŠ›è¢«æ¸©å’Œåœ°å¼ºåŒ–ï¼ˆé€šè¿‡æƒé‡ï¼‰ï¼Œè€Œä¸æ˜¯è¢«æ¿€è¿›åœ°è¿½æ±‚
- Online ReinFlow å¯ä»¥åœ¨è¿™ä¸ª"å¥åº·çš„ shortcut"åŸºç¡€ä¸Šï¼Œç”¨çœŸå® reward æŒ‡å¯¼å¤§æ­¥æ¢ç´¢

#### 3. **æ¢¯åº¦æµå‘çš„ç²¾å¿ƒè®¾è®¡** (Critical!)

```
Stage 1: Flow matchingï¼ˆçº¯ BCï¼‰
  âˆ‡_Î¸ L_flow â†’ åªä¼˜åŒ– velocity ç½‘ç»œï¼Œæ—  Q åé¦ˆ

Stage 2: AW-ShortCutï¼ˆç¦»çº¿ RLï¼‰
  âˆ‡_Î¸ L_AW_flow = âˆ‡_Î¸ (w * L_flow)
             = âˆ‚w/âˆ‚Q * âˆ‚Q/âˆ‚a * âˆ‡_a L_flow + w * âˆ‡_Î¸ L_flow
                    â†‘ æ— æ¢¯åº¦ï¼ˆdetachï¼‰      â†‘ ä¸»æ¢¯åº¦ï¼ˆæƒé‡è°ƒæ•´ï¼‰
  
  ç»“æœï¼šPolicy å‚æ•° Î¸ çš„æ¢¯åº¦å®Œå…¨æ¥è‡ªï¼š
    1. BC lossï¼ˆä¸»åŠ›ï¼‰+ æƒé‡è°ƒåˆ¶ï¼ˆQä¿¡æ¯ï¼‰
    2. ä¸ç›´æ¥æœ€å¤§åŒ– Qï¼ˆé¿å… CPQL çš„é—®é¢˜ï¼‰

Stage 3: ReinFlowï¼ˆåœ¨çº¿ RLï¼‰
  âˆ‡_Î¸ L_PPO + âˆ‡_Ïˆ L_actor_critic
  
  æ­¤æ—¶ Policy å·²ç»æ˜¯ä¸€ä¸ª"ç¨³å®šçš„ã€å‡ ä½•æ­£ç¡®çš„"ODE solverï¼Œ
  RL å¯ä»¥å®‰å…¨åœ°ç”¨çœŸå® reward åšå¾®è°ƒ
```

è¿™ä¸ªè®¾è®¡ç¡®ä¿äº†ï¼š
- **Stage 1 â†’ 2 çš„è¿ç»­æ€§**ï¼šShortcut ç»“æ„è¢«ä¿ç•™
- **Stage 2 çš„ç¨³å®šæ€§**ï¼šCritic å­¦ä¹ æœ‰ç•Œï¼Œä¸çˆ†ç‚¸
- **Stage 2 â†’ 3 çš„å¹³é¡ºè¿‡æ¸¡**ï¼šPolicy å·²ç»æ˜¯å¥½çš„åˆå§‹åŒ–ï¼ŒRL å¾®è°ƒå®¹æ˜“æˆåŠŸ

---

## 6ï¸âƒ£ å¤±è´¥æ¡ˆä¾‹ä¸è°ƒè¯•æŒ‡å—

### Case 1: Critic Loss çˆ†ç‚¸

**ç—‡çŠ¶**ï¼š
```
Iter 1000:  critic_loss = 0.5
Iter 2000:  critic_loss = 2.1
Iter 3000:  critic_loss = 45.0  â† çˆ†ç‚¸
```

**åŸå› åˆ†æ**ï¼š
1. `reward_scale` å¤ªå¤§ â†’ Q target å¢é•¿å¤ªå¿« â†’ MSE loss å¾ˆå¤§
2. `q_target_clip` ä¸å¤Ÿä¸¥æ ¼ â†’ target Q æœ‰ç¦»ç¾¤å€¼
3. Policy è·‘å‡ºåˆ†å¸ƒ â†’ next actions å¾ˆå¥‡æ€ª â†’ target ä¼°è®¡å·®

**è°ƒæ•´**ï¼š
```python
# è¯•è¯•è¿™ä¸ªé¡ºåºï¼š
1. â†“ reward_scale:  0.1 â†’ 0.05
2. â†“ beta:          0.5 â†’ 0.3  (å¼±åŒ– AW weighting)
3. â†‘ warmup_iters:  2000 â†’ 5000 (çº¯ BC é¢„çƒ­æ›´é•¿)
4. â†“ q_target_clip: 100 â†’ 50
```

### Case 2: Success æ— æå‡ï¼ˆç”šè‡³ä¸‹é™ï¼‰

**ç—‡çŠ¶**ï¼š
```
Stage 1 baseline: success = 0.42
Stage 2 after 50k steps: success = 0.40  â† å˜å·®äº†
```

**åŸå› åˆ†æ**ï¼š
1. AW weighting å¤ªå¼± â†’ Policy æ²¡æœ‰æ„Ÿå—åˆ° Q çš„æŒ‡å¯¼
2. Reward ä¿¡å·é”™è¯¯ â†’ demo çš„ Q ä¼°è®¡æœ¬èº«å°±ä¸å¯¹
3. Critic è¿˜æ²¡æ ¡å‡†å¥½ â†’ Q ä¼°è®¡å™ªå£°å¤ªå¤§

**è°ƒæ•´**ï¼š
```python
# å…ˆç¡®è®¤ reward/Q çš„åˆç†æ€§ï¼š
- æ‰“å°å‡º demo çš„ Q åˆ†å¸ƒ
- æ£€æŸ¥ï¼šmax(Q) / min(Q) æ˜¯å¦åˆç†ï¼ˆåº”è¯¥æ˜¯ 5-20 å€è€Œé 100+ å€ï¼‰
- å¦‚æœä¸å¯¹ï¼Œæ£€æŸ¥ reward è®¡ç®—é€»è¾‘

# å¦‚æœ reward æ­£ç¡®ï¼Œè°ƒå‚ï¼š
1. â†‘ beta: 0.5 â†’ 1.0 (åŠ å¼º AW weighting)
2. â†“ weight_clip: 10.0 â†’ 5.0 (è®©é«˜Qæ ·æœ¬çš„æƒé‡é™åˆ¶æ›´ç´§)
3. å»¶é•¿ stage 2 çš„ iters: 50k â†’ 100k
```

### Case 3: Policy å’Œ Critic äº§ç”Ÿ "Adversarial" åŠ¨æ€

**ç—‡çŠ¶**ï¼š
```
Iter 5000:  policy_loss â†“, critic_loss â†‘
Iter 10000: policy_loss â†‘, critic_loss â†“
â†’ ä¸¤ä¸ª loss äº’ç›¸æŠµæ¶ˆï¼Œéƒ½æ— æ³•æ”¶æ•›
```

**åŸå› åˆ†æ**ï¼š
- Policy å’Œ Critic å­¦ä¹ ç‡ä¸å¹³è¡¡
- æˆ–è€… critic çš„ target æ›´æ–°ï¼ˆsoft updateï¼‰å¤ªå¿«

**è°ƒæ•´**ï¼š
```python
# æ–¹æ¡ˆ 1: é™ä½å­¦ä¹ ç‡ï¼ŒåŒæ­¥æ›´æ–°
lr_policy = 5e-5 (â†“ from 1e-4)
lr_critic = 5e-5

# æ–¹æ¡ˆ 2: ä¿®æ”¹æ›´æ–°é¢‘ç‡
update_critic_freq = 2  # æ¯ 2 ä¸ª policy step æ›´æ–°ä¸€æ¬¡ critic
soft_update_tau = 0.001 (â†“ from 0.005)

# æ–¹æ¡ˆ 3: äº¤æ›¿å†»ç»“
# å‰ 10k steps: åªæ›´æ–° criticï¼Œå†»ç»“ policy
# ä¸­é—´ 20k steps: åŒæ—¶æ›´æ–°
# å 20k steps: åªæ›´æ–° policyï¼Œå†»ç»“ critic
```

---

## ğŸ“ è®ºæ–‡å†™ä½œæŒ‡å¼•

### ç« èŠ‚ç»„ç»‡

```markdown
## Offline RL è¿‡æ¸¡å±‚ï¼šAdvantage-Weighted ShortCut Flow

### 4.1 Motivation
- ç›´æ¥ä» BC è·³åˆ° Online RL é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šcritic æœªæ ¡å‡†ï¼Œpolicy å¯èƒ½è¢« RL æ¨å‡ºåˆ†å¸ƒ
- Advantage-weighted BC æ˜¯ç¦»çº¿ RL çš„æ—¢å¾€æœ€ä½³å®è·µï¼ˆAWAC, IQLï¼‰
- æˆ‘ä»¬å°†å…¶ä¸ ShortCut Flow ç»“åˆï¼Œå¾—åˆ° AW-ShortCut Flow

### 4.2 Method
#### 4.2.1 Policy Loss: Advantage-Weighted Flow Matching
- Equation: w_i = clip(exp(Î² A_i), w_max), where A_i = Q_i - baseline
- Loss: L_AW_flow = Î£ w_i ||v_pred(x_t^i, t^i, d^i) - v_target^i||^2

#### 4.2.2 Consistency Loss: Advantage-Weighted Shortcut
- åŒæ ·çš„æƒé‡åº”ç”¨åˆ° shortcut consistency loss

#### 4.2.3 Critic Loss: SMDP Bellman
- Double Q-learning with soft target updates

### 4.3 Experimental Setup
- Stage 1 baseline: ShortCut Flow BC (best config from sweep)
- Stage 2: AW-ShortCut Flow (50k iterations)
- Hyperparameters: Î², weight_clip, reward_scale

### 4.4 Results
- Offline RL stage æå‡ 5-10% success rate
- Critic ç¨³å®šå­¦ä¹ ï¼Œæ— çˆ†ç‚¸ç°è±¡
- ä¸º online ReinFlow æä¾› warm-start

### 4.5 Ablation (Optional)
- Effect of Î²
- Effect of weight_clip
- Effect of reward_scale
```

### å…³é”®è®ºè¿°

> Unlike naive Q-learning approaches (CPQL) which risk distributional shift and critic instability, our Advantage-Weighted ShortCut Flow keeps the policy on the offline demonstration manifold while using Q-values to modulate which samples to prioritize. This design leverages the mathematical structure of ShortCut Flow as a local ODE solver approximation: small steps and velocity targets ensure the policy remains in a reliable regime, while EMA teachers and conservative double-Q learning keep critic estimates bounded.

---

## ğŸ“‹ å®ç°æ¸…å•

- [ ] åˆ›å»º `algorithms/aw_shortcut_flow.py` æ–‡ä»¶
  - [ ] å®ç° `AWShortCutFlowAgent.__init__`
  - [ ] å®ç° `_compute_flow_loss`ï¼ˆAW-weightedï¼‰
  - [ ] å®ç° `_compute_shortcut_loss`ï¼ˆAW-weightedï¼‰
  - [ ] å®ç° `_compute_critic_loss`ï¼ˆSMDP Bellmanï¼‰
  - [ ] å®ç° `compute_loss` æ–¹æ³•æ±‡æ€»
  - [ ] å®ç° `get_action` æ–¹æ³•
  - [ ] å®ç° `update_ema` å’Œ `update_target` æ–¹æ³•
- [ ] ä¿®æ”¹ `train_offline_rl.py`
  - [ ] æ·»åŠ  AW-SCF ç›¸å…³å‘½ä»¤è¡Œå‚æ•°ï¼ˆbeta, weight_clip, etc.)
  - [ ] åœ¨ `create_agent` ä¸­æ·»åŠ  aw_shortcut_flow case
  - [ ] éªŒè¯æ•°æ®åŠ è½½å’Œ reward è®¡ç®—
- [ ] ä¿®æ”¹ `algorithms/__init__.py` å¯¼å‡º `AWShortCutFlowAgent`
- [ ] åˆ›å»º sweep è„šæœ¬ `sweep_aw_scf_offline_rl.sh`
- [ ] æµ‹è¯•ä¸‰é˜¶æ®µç®¡é“
  - [ ] Stage 1ï¼šè¿è¡Œ ShortCut Flow BC åˆ°æ”¶æ•›ï¼Œä¿å­˜ checkpoint
  - [ ] Stage 2ï¼šä» Stage 1 checkpoint åŠ è½½ï¼Œè¿è¡Œ AW-SCF
  - [ ] éªŒè¯ success rate æœ‰æå‡ï¼Œcritic ä¸çˆ†ç‚¸
  - [ ] Stage 3ï¼šä» Stage 2 checkpoint åˆå§‹åŒ– ReinFlowï¼ˆfuture workï¼‰

---

## å‚è€ƒæ–‡çŒ®ä¸ç›¸å…³å·¥ä½œ

- **AWAC**: Advantage-Weighted Actor-Critic (https://arxiv.org/abs/2006.09359)
- **IQL**: Implicit Q-Learning (https://arxiv.org/abs/2110.06169)
- **CPQL**: ä½ ä¹‹å‰çš„å®éªŒä¸­å‘ç°ç›´æ¥ Q-learning ä¸ç¨³å®š
- **ShortCut Flow / ReinFlow**: æœ¬å·¥ä½œçš„ä¸»è¦æ–¹æ³•
- **SMDP å­¦ä¹ **: å¤„ç†å˜é•¿ action chunks çš„æ ‡å‡†æ¡†æ¶

