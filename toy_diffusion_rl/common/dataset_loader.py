"""
Dataset Loaders for Offline RL Pretraining.

This module provides PyTorch Dataset classes for loading offline demonstration
data in HDF5 format, supporting state-only, image-only, and multimodal observations.
"""

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Optional, Callable, Dict, Any, Tuple, Union
from pathlib import Path


class PickAndPlaceOfflineDataset(Dataset):
    """PyTorch Dataset for Pick-and-Place offline demonstrations.
    
    Loads data from HDF5 files generated by generate_pick_and_place_dataset.py.
    Supports state-only, image-only, and state+image observation modes.
    
    Args:
        h5_path: Path to HDF5 dataset file
        obs_mode: Observation mode ("state", "image", or "state_image")
        transform: Optional transform for images (e.g., normalization, augmentation)
        normalize_state: Whether to normalize state observations
        load_to_memory: Whether to load entire dataset to memory (faster but uses more RAM)
    
    Example:
        >>> dataset = PickAndPlaceOfflineDataset("data/fetch_state_image.h5", obs_mode="state_image")
        >>> sample = dataset[0]
        >>> print(sample["obs"].shape, sample["image"].shape, sample["action"].shape)
        
        >>> # Create DataLoader
        >>> dataloader = dataset.make_dataloader(batch_size=64, shuffle=True)
        >>> for batch in dataloader:
        ...     obs, action = batch["obs"], batch["action"]
    """
    
    def __init__(
        self,
        h5_path: str,
        obs_mode: str = "state",
        transform: Optional[Callable] = None,
        normalize_state: bool = False,
        load_to_memory: bool = True,
    ):
        super().__init__()
        
        self.h5_path = Path(h5_path)
        self.obs_mode = obs_mode
        self.transform = transform
        self.normalize_state = normalize_state
        self.load_to_memory = load_to_memory
        
        # Validate obs_mode
        assert obs_mode in ["state", "image", "state_image"], \
            f"obs_mode must be 'state', 'image', or 'state_image', got {obs_mode}"
        
        # Open HDF5 file and load metadata
        with h5py.File(h5_path, "r") as f:
            self.metadata = dict(f.attrs)
            self.length = len(f["actions"])
            
            # Check available data
            self.has_obs = "obs" in f
            self.has_images = "images" in f
            
            # Validate data availability for obs_mode
            if obs_mode in ["state", "state_image"] and not self.has_obs:
                raise ValueError(f"Dataset does not contain state observations for obs_mode='{obs_mode}'")
            if obs_mode in ["image", "state_image"] and not self.has_images:
                raise ValueError(f"Dataset does not contain images for obs_mode='{obs_mode}'")
            
            # Load data to memory if requested
            if load_to_memory:
                self.actions = f["actions"][:]
                self.rewards = f["rewards"][:]
                self.dones = f["dones"][:]
                
                if self.has_obs:
                    self.obs = f["obs"][:]
                    self.next_obs = f["next_obs"][:]
                    
                    # Compute normalization stats
                    if normalize_state:
                        self.obs_mean = self.obs.mean(axis=0)
                        self.obs_std = self.obs.std(axis=0) + 1e-8
                
                if self.has_images:
                    self.images = f["images"][:]
                    self.next_images = f["next_images"][:]
            else:
                # Store file reference for lazy loading
                self._file = None
        
        # Get shapes
        with h5py.File(h5_path, "r") as f:
            self.action_dim = f["actions"].shape[1]
            if self.has_obs:
                self.state_dim = f["obs"].shape[1]
            else:
                self.state_dim = None
            if self.has_images:
                self.image_shape = f["images"].shape[1:]  # (H, W, C)
            else:
                self.image_shape = None
    
    @property
    def file(self) -> h5py.File:
        """Lazy file handle for non-memory mode."""
        if self._file is None:
            self._file = h5py.File(self.h5_path, "r")
        return self._file
    
    def __len__(self) -> int:
        return self.length
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get a single sample.
        
        Returns:
            Dictionary containing:
                - "action": Action tensor (action_dim,)
                - "reward": Reward scalar
                - "done": Done flag
                
            For state mode:
                - "obs": State observation (state_dim,)
                - "next_obs": Next state (state_dim,)
                
            For image mode:
                - "image": Image tensor (C, H, W) normalized to [0, 1]
                - "next_image": Next image tensor
                
            For state_image mode:
                - All of the above
        """
        if self.load_to_memory:
            return self._get_from_memory(idx)
        else:
            return self._get_from_file(idx)
    
    def _get_from_memory(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get sample from memory."""
        sample = {
            "action": torch.from_numpy(self.actions[idx]),
            "reward": torch.tensor(self.rewards[idx], dtype=torch.float32),
            "done": torch.tensor(self.dones[idx], dtype=torch.bool),
        }
        
        # Add state observations
        if self.obs_mode in ["state", "state_image"]:
            obs = self.obs[idx].copy()
            next_obs = self.next_obs[idx].copy()
            
            if self.normalize_state:
                obs = (obs - self.obs_mean) / self.obs_std
                next_obs = (next_obs - self.obs_mean) / self.obs_std
            
            sample["obs"] = torch.from_numpy(obs).float()
            sample["next_obs"] = torch.from_numpy(next_obs).float()
        
        # Add image observations
        if self.obs_mode in ["image", "state_image"]:
            image = self.images[idx]
            next_image = self.next_images[idx]
            
            # Apply transform if provided
            if self.transform:
                image = self.transform(image)
                next_image = self.transform(next_image)
            
            # Convert to tensor: (H, W, C) -> (C, H, W), normalize to [0, 1]
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            next_image = torch.from_numpy(next_image).permute(2, 0, 1).float() / 255.0
            
            sample["image"] = image
            sample["next_image"] = next_image
        
        return sample
    
    def _get_from_file(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get sample by reading from file (slower but memory efficient)."""
        f = self.file
        
        sample = {
            "action": torch.from_numpy(f["actions"][idx]),
            "reward": torch.tensor(f["rewards"][idx], dtype=torch.float32),
            "done": torch.tensor(f["dones"][idx], dtype=torch.bool),
        }
        
        if self.obs_mode in ["state", "state_image"]:
            sample["obs"] = torch.from_numpy(f["obs"][idx]).float()
            sample["next_obs"] = torch.from_numpy(f["next_obs"][idx]).float()
        
        if self.obs_mode in ["image", "state_image"]:
            image = f["images"][idx]
            next_image = f["next_images"][idx]
            
            if self.transform:
                image = self.transform(image)
                next_image = self.transform(next_image)
            
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            next_image = torch.from_numpy(next_image).permute(2, 0, 1).float() / 255.0
            
            sample["image"] = image
            sample["next_image"] = next_image
        
        return sample
    
    def make_dataloader(
        self,
        batch_size: int = 64,
        shuffle: bool = True,
        num_workers: int = 0,
        pin_memory: bool = True,
        drop_last: bool = True,
    ) -> DataLoader:
        """Create a PyTorch DataLoader.
        
        Args:
            batch_size: Batch size
            shuffle: Whether to shuffle data
            num_workers: Number of worker processes
            pin_memory: Whether to pin memory for faster GPU transfer
            drop_last: Whether to drop last incomplete batch
        
        Returns:
            PyTorch DataLoader
        """
        return DataLoader(
            self,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
        )
    
    def get_all_obs(self) -> torch.Tensor:
        """Get all state observations as a tensor.
        
        Useful for computing normalization statistics or BC training.
        
        Returns:
            Tensor of shape (N, state_dim)
        """
        if not self.has_obs:
            raise ValueError("Dataset does not contain state observations")
        
        if self.load_to_memory:
            obs = self.obs
        else:
            obs = self.file["obs"][:]
        
        return torch.from_numpy(obs).float()
    
    def get_all_actions(self) -> torch.Tensor:
        """Get all actions as a tensor.
        
        Returns:
            Tensor of shape (N, action_dim)
        """
        if self.load_to_memory:
            actions = self.actions
        else:
            actions = self.file["actions"][:]
        
        return torch.from_numpy(actions).float()
    
    def get_normalization_stats(self) -> Dict[str, torch.Tensor]:
        """Compute normalization statistics for observations and actions.
        
        Returns:
            Dictionary with obs_mean, obs_std, action_mean, action_std
        """
        stats = {}
        
        if self.has_obs:
            obs = self.get_all_obs()
            stats["obs_mean"] = obs.mean(dim=0)
            stats["obs_std"] = obs.std(dim=0) + 1e-8
        
        actions = self.get_all_actions()
        stats["action_mean"] = actions.mean(dim=0)
        stats["action_std"] = actions.std(dim=0) + 1e-8
        
        return stats
    
    def __repr__(self) -> str:
        return (
            f"PickAndPlaceOfflineDataset(\n"
            f"  path={self.h5_path},\n"
            f"  obs_mode='{self.obs_mode}',\n"
            f"  length={self.length},\n"
            f"  state_dim={self.state_dim},\n"
            f"  image_shape={self.image_shape},\n"
            f"  action_dim={self.action_dim},\n"
            f")"
        )


class ManiSkillOfflineDataset(PickAndPlaceOfflineDataset):
    """PyTorch Dataset for ManiSkill3 offline demonstrations.
    
    Extends PickAndPlaceOfflineDataset to handle ManiSkill3-specific data formats.
    Compatible with datasets generated by scripts/generate_maniskill_dataset.py.
    
    Key differences from Pick-and-Place:
    - PickCube-v1 task with pd_ee_delta_pose control (7D actions)
    - Different state observation structure
    - Same image format (H, W, C) in uint8
    
    Args:
        h5_path: Path to HDF5 dataset file
        obs_mode: Observation mode ("state", "image", or "state_image")
        transform: Optional transform for images
        normalize_state: Whether to normalize state observations
        load_to_memory: Whether to load entire dataset to memory
        
    Example:
        >>> dataset = ManiSkillOfflineDataset("data/maniskill_pickcube.h5", obs_mode="state_image")
        >>> sample = dataset[0]
        >>> print(sample["obs"].shape, sample["image"].shape, sample["action"].shape)
    """
    
    def __init__(
        self,
        h5_path: str,
        obs_mode: str = "state_image",
        transform: Optional[Callable] = None,
        normalize_state: bool = False,
        load_to_memory: bool = True,
    ):
        # Call parent constructor
        super().__init__(
            h5_path=h5_path,
            obs_mode=obs_mode,
            transform=transform,
            normalize_state=normalize_state,
            load_to_memory=load_to_memory,
        )
        
        # Read ManiSkill-specific metadata
        with h5py.File(h5_path, "r") as f:
            self.task = f.attrs.get("task", "PickCube-v1")
            self.control_mode = f.attrs.get("control_mode", "pd_ee_delta_pose")
    
    def __repr__(self) -> str:
        return (
            f"ManiSkillOfflineDataset(\n"
            f"  path={self.h5_path},\n"
            f"  task='{self.task}',\n"
            f"  control_mode='{self.control_mode}',\n"
            f"  obs_mode='{self.obs_mode}',\n"
            f"  length={self.length},\n"
            f"  state_dim={self.state_dim},\n"
            f"  image_shape={self.image_shape},\n"
            f"  action_dim={self.action_dim},\n"
            f")"
        )


class TrajectoryDataset(Dataset):
    """Dataset that returns full trajectories instead of single transitions.
    
    Useful for sequence models or trajectory-level processing.
    
    Args:
        h5_path: Path to HDF5 dataset file
        obs_mode: Observation mode ("state", "image", or "state_image")
        sequence_length: Length of trajectory sequences to return
        stride: Stride between sequence start points
    """
    
    def __init__(
        self,
        h5_path: str,
        obs_mode: str = "state",
        sequence_length: int = 50,
        stride: int = 1,
    ):
        super().__init__()
        
        self.h5_path = h5_path
        self.obs_mode = obs_mode
        self.sequence_length = sequence_length
        self.stride = stride
        
        # Load data
        with h5py.File(h5_path, "r") as f:
            self.episode_ids = f["episode_ids"][:]
            self.timesteps = f["timesteps"][:]
            self.actions = f["actions"][:]
            self.rewards = f["rewards"][:]
            self.dones = f["dones"][:]
            
            if "obs" in f:
                self.obs = f["obs"][:]
                self.next_obs = f["next_obs"][:]
            else:
                self.obs = None
            
            if "images" in f:
                self.images = f["images"][:]
                self.next_images = f["next_images"][:]
            else:
                self.images = None
        
        # Find valid sequence start indices
        self.valid_indices = self._find_valid_indices()
    
    def _find_valid_indices(self) -> np.ndarray:
        """Find indices where a full sequence can be extracted."""
        valid = []
        
        for i in range(0, len(self.episode_ids) - self.sequence_length + 1, self.stride):
            # Check if all timesteps are from the same episode
            episode_id = self.episode_ids[i]
            if np.all(self.episode_ids[i:i + self.sequence_length] == episode_id):
                valid.append(i)
        
        return np.array(valid)
    
    def __len__(self) -> int:
        return len(self.valid_indices)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get a trajectory sequence."""
        start = self.valid_indices[idx]
        end = start + self.sequence_length
        
        sample = {
            "actions": torch.from_numpy(self.actions[start:end]),
            "rewards": torch.from_numpy(self.rewards[start:end]),
            "dones": torch.from_numpy(self.dones[start:end]),
        }
        
        if self.obs is not None and self.obs_mode in ["state", "state_image"]:
            sample["obs"] = torch.from_numpy(self.obs[start:end]).float()
            sample["next_obs"] = torch.from_numpy(self.next_obs[start:end]).float()
        
        if self.images is not None and self.obs_mode in ["image", "state_image"]:
            images = torch.from_numpy(self.images[start:end]).permute(0, 3, 1, 2).float() / 255.0
            next_images = torch.from_numpy(self.next_images[start:end]).permute(0, 3, 1, 2).float() / 255.0
            sample["images"] = images
            sample["next_images"] = next_images
        
        return sample


def make_dataloader(
    h5_path: str,
    obs_mode: str = "state",
    batch_size: int = 64,
    shuffle: bool = True,
    num_workers: int = 0,
    dataset_type: str = "auto",
    **kwargs
) -> DataLoader:
    """Convenience function to create a DataLoader directly from HDF5 file.
    
    Args:
        h5_path: Path to HDF5 dataset file
        obs_mode: Observation mode
        batch_size: Batch size
        shuffle: Whether to shuffle
        num_workers: Number of workers
        dataset_type: Dataset type ("auto", "pick_and_place", or "maniskill")
                     "auto" will detect from file metadata
        **kwargs: Additional arguments for Dataset class
    
    Returns:
        PyTorch DataLoader
    """
    # Auto-detect dataset type
    if dataset_type == "auto":
        with h5py.File(h5_path, "r") as f:
            task = f.attrs.get("task", "")
            if isinstance(task, bytes):
                task = task.decode("utf-8")
            if "PickCube" in task or "maniskill" in h5_path.lower():
                dataset_type = "maniskill"
            else:
                dataset_type = "pick_and_place"
    
    if dataset_type == "maniskill":
        dataset = ManiSkillOfflineDataset(h5_path, obs_mode=obs_mode, **kwargs)
    else:
        dataset = PickAndPlaceOfflineDataset(h5_path, obs_mode=obs_mode, **kwargs)
    
    return dataset.make_dataloader(
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
    )


if __name__ == "__main__":
    # Test dataset loading
    import sys
    from pathlib import Path
    
    # Check if test dataset exists
    test_path = Path("data/test_state_image.h5")
    if not test_path.exists():
        print("Test dataset not found. Run generate_pick_and_place_dataset.py first.")
        sys.exit(1)
    
    print("Testing PickAndPlaceOfflineDataset...")
    
    # Test state mode
    print("\n1. Testing state mode:")
    dataset = PickAndPlaceOfflineDataset(str(test_path), obs_mode="state")
    print(f"   {dataset}")
    sample = dataset[0]
    print(f"   Sample keys: {sample.keys()}")
    print(f"   obs shape: {sample['obs'].shape}")
    print(f"   action shape: {sample['action'].shape}")
    
    # Test state_image mode
    print("\n2. Testing state_image mode:")
    dataset = PickAndPlaceOfflineDataset(str(test_path), obs_mode="state_image")
    sample = dataset[0]
    print(f"   Sample keys: {sample.keys()}")
    print(f"   obs shape: {sample['obs'].shape}")
    print(f"   image shape: {sample['image'].shape}")
    print(f"   image range: [{sample['image'].min():.3f}, {sample['image'].max():.3f}]")
    
    # Test image mode
    print("\n3. Testing image mode:")
    dataset = PickAndPlaceOfflineDataset(str(test_path), obs_mode="image")
    sample = dataset[0]
    print(f"   Sample keys: {sample.keys()}")
    print(f"   image shape: {sample['image'].shape}")
    
    # Test DataLoader
    print("\n4. Testing DataLoader:")
    dataloader = dataset.make_dataloader(batch_size=32, shuffle=True)
    batch = next(iter(dataloader))
    print(f"   Batch image shape: {batch['image'].shape}")
    print(f"   Batch action shape: {batch['action'].shape}")
    
    # Test normalization stats
    print("\n5. Testing normalization stats:")
    dataset = PickAndPlaceOfflineDataset(str(test_path), obs_mode="state")
    stats = dataset.get_normalization_stats()
    print(f"   obs_mean shape: {stats['obs_mean'].shape}")
    print(f"   action_std shape: {stats['action_std'].shape}")
    
    print("\nâœ“ All dataset loader tests passed!")
