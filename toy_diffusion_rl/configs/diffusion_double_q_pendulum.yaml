# Diffusion Double Q Learning configuration
# Based on: https://github.com/Zhendong-Wang/Diffusion-Policies-for-Offline-RL

_base_: "base.yaml"

algorithm: "diffusion_double_q"

# Environment
env:
  name: "point_mass_2d"
  max_episode_steps: 200

# Training settings for offline RL
training:
  seed: 42
  total_steps: 50000
  batch_size: 256
  eval_interval: 2000
  
# Expert data collection (mixed with suboptimal data)
expert_data:
  num_episodes: 100
  noise_std: 0.2  # More noise for diverse data

# Diffusion settings
diffusion:
  num_diffusion_steps: 10  # Keep small for speed
  noise_schedule: "linear"
  beta_start: 0.0001
  beta_end: 0.02

# RL settings
rl:
  gamma: 0.99
  tau: 0.005
  alpha: 1.0  # Q-value weight in actor loss
  bc_weight: 1.0  # BC loss weight

# Network architecture
network:
  hidden_dims: [256, 256]
  
# Optimizer
optimizer:
  learning_rate_actor: 3.0e-4
  learning_rate_critic: 3.0e-4
