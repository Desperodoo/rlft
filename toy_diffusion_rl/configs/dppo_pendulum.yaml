# DPPO (Diffusion Policy Policy Optimization) configuration
# Based on: https://github.com/irom-princeton/dppo

_base_: "base.yaml"

algorithm: "dppo"

# Environment
env:
  name: "point_mass_2d"
  max_episode_steps: 200

# Training settings (online RL)
training:
  seed: 42
  total_iterations: 100  # Number of PPO iterations
  rollout_steps: 2048  # Steps per rollout
  eval_interval: 10
  
# Optional: BC pretraining
pretrain:
  enabled: true
  num_steps: 2000
  expert_episodes: 50
  expert_noise: 0.1

# Diffusion settings
diffusion:
  num_diffusion_steps: 5  # Keep small for on-policy
  noise_schedule: "linear"

# PPO settings
ppo:
  clip_ratio: 0.2
  ppo_epochs: 10
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5

# Network architecture
network:
  hidden_dims: [256, 256]
  
# Optimizer
optimizer:
  learning_rate: 3.0e-4
