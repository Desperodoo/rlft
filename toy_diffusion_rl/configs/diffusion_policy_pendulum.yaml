# Diffusion Policy configuration for behavior cloning
# Based on: https://github.com/real-stanford/diffusion_policy

# Inherit from base config
_base_: "base.yaml"

algorithm: "diffusion_policy"

# Environment
env:
  name: "point_mass_2d"
  max_episode_steps: 200

# Training settings for offline/BC
training:
  seed: 42
  total_steps: 20000
  batch_size: 256
  eval_interval: 1000
  eval_episodes: 10
  
# Expert data collection
expert_data:
  num_episodes: 100
  noise_std: 0.1
  
# Diffusion-specific settings
diffusion:
  num_diffusion_steps: 100
  noise_schedule: "linear"  # "linear" or "cosine"
  beta_start: 0.0001
  beta_end: 0.02
  
# Network architecture
network:
  hidden_dims: [256, 256, 256]
  time_embed_dim: 64

# Optimizer
optimizer:
  learning_rate: 1.0e-4
  
# Inference settings
inference:
  num_inference_steps: 10  # Can be less than training steps (DDIM-style)
  use_ema: true
  ema_decay: 0.999
