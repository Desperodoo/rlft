# AWSC: Advantage-Weighted ShortCut Flow é¡¹ç›®æ€»ç»“

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ **Offlineâ†’Online RL è®­ç»ƒæµç¨‹**ï¼Œä¸“ä¸ºé«˜ç»´å›¾åƒè¾“å…¥ï¼ˆRGBï¼‰å’ŒåŠ¨ä½œåˆ†å—ï¼ˆAction Chunkingï¼‰åœºæ™¯è®¾è®¡ã€‚æ ¸å¿ƒç®—æ³•æ˜¯ **AWSC (Advantage-Weighted ShortCut Flow)**ï¼Œç»“åˆäº† Flow Matching ç­–ç•¥å’Œä¼˜åŠ¿åŠ æƒï¼ˆAdvantage Weightingï¼‰æœºåˆ¶ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹© AWSCï¼Ÿ

åœ¨ **å›¾åƒè¾“å…¥ + Action Chunking** æ¡ä»¶ä¸‹ï¼Œä¼ ç»Ÿçš„ Q-Learning ç±»æ–¹æ¡ˆå­˜åœ¨ä¸¥é‡çš„ç¨³å®šæ€§é—®é¢˜ï¼š

| æ–¹æ¡ˆ | é—®é¢˜ |
|------|------|
| **ç›´æ¥ Q æœ€å¤§åŒ–** (CPQL, DQL) | Policy æ˜“è·‘åˆ° OOD åŒºåŸŸï¼ŒCritic ä¼°è®¡çˆ†ç‚¸ |
| **Diffusion-QL** | Action chunking ä¸‹ Q-gradient ä¸ç¨³å®š |
| **SAC + Action Chunking** | é«˜ç»´åŠ¨ä½œç©ºé—´ (action_horizon Ã— action_dim) æ¢ç´¢å›°éš¾ |

å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ **Advantage-Weighted BC** çš„æ€è·¯ï¼š
- Policy ä¿æŒåœ¨ demo åˆ†å¸ƒé™„è¿‘ï¼ˆä¸åš Q æœ€å¤§åŒ–ï¼‰
- Q å€¼ä»…ç”¨äºåŠ æƒ BC æ ·æœ¬
- Critic åœ¨æ•°æ®æ”¯æ’‘åŒºåŸŸå­¦ä¹ ï¼Œä¼°è®¡è¯¯å·®æœ‰ç•Œ

---

## ğŸ—ï¸ ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Stage 1: ShortCut Flow BC                     â”‚
â”‚  çº¯ BC é¢„è®­ç»ƒï¼Œå­¦ä¹  demo åˆ†å¸ƒçš„ flow matching                      â”‚
â”‚  è„šæœ¬: train_offline_rl.py --algorithm shortcut_flow             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Stage 2: AW-ShortCut Flow Offline RL              â”‚
â”‚  ç¦»çº¿ RL å¾®è°ƒï¼ŒQ-weighted BC åˆå§‹åŒ– Critic                        â”‚
â”‚  è„šæœ¬: train_offline_rl.py --algorithm aw_shortcut_flow          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Stage 3: AWSC Online RL (RLPD)                  â”‚
â”‚  åœ¨çº¿ RL å¾®è°ƒï¼Œ50% online + 50% offline æ•°æ®æ··åˆ                   â”‚
â”‚  è„šæœ¬: train_rlpd_online.py --algorithm awsc                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 1: ShortCut Flow BC
- **ç›®æ ‡**ï¼šå­¦ä¹ ä¸“å®¶æ¼”ç¤ºçš„åŠ¨ä½œåˆ†å¸ƒ
- **ç®—æ³•**ï¼šFlow Matching with ShortCutï¼ˆå±€éƒ¨ ODE æ±‚è§£å™¨è¿‘ä¼¼ï¼‰
- **è¾“å‡º**ï¼šé¢„è®­ç»ƒçš„ `velocity_net`

### Stage 2: AW-ShortCut Flow Offline RL
- **ç›®æ ‡**ï¼šå¼•å…¥ Q å‡½æ•°ï¼Œåœ¨ demo åˆ†å¸ƒå†…è¿›è¡Œä¼˜åŠ¿åŠ æƒ
- **ç®—æ³•**ï¼šAWAC-style Q-weighted BC + SMDP Bellman Critic
- **å…³é”®**ï¼šä½¿ç”¨ **EnsembleQNetwork** ç¡®ä¿ä¸ Stage 3 å…¼å®¹
- **è¾“å‡º**ï¼šå¾®è°ƒçš„ `velocity_net` + æ ¡å‡†çš„ `critic`

### Stage 3: AWSC Online RL
- **ç›®æ ‡**ï¼šç¯å¢ƒäº¤äº’ï¼Œåœ¨çº¿æ•°æ®å¢å¼º
- **ç®—æ³•**ï¼šRLPD (Reinforcement Learning with Prior Data)
- **æ•°æ®æ··åˆ**ï¼š`online_ratio=0.5`ï¼ˆ50% åœ¨çº¿ + 50% ç¦»çº¿ï¼‰
- **å…³é”®ç‰¹æ€§**ï¼šPolicy-Critic æ•°æ®åˆ†ç¦»

---

## ğŸ”§ æ ¸å¿ƒç®—æ³•ç»„ä»¶

### 1. ShortCut Flow Policy

ShortCut Flow æ˜¯ä¸€ç§é«˜æ•ˆçš„ Flow Matching ç­–ç•¥ï¼Œé€šè¿‡å­¦ä¹ å±€éƒ¨ ODE æ±‚è§£å™¨æ¥åŠ é€Ÿé‡‡æ ·ï¼š

```python
class ShortCutVelocityUNet1D(nn.Module):
    """
    Velocity network with step size conditioning.
    
    è¾“å…¥:
        - sample: (B, pred_horizon, action_dim) å½“å‰å™ªå£°åŠ¨ä½œ
        - timestep: (B,) æ‰©æ•£æ—¶é—´æ­¥ t âˆˆ [0, 1]
        - step_size: (B,) æ­¥é•¿ d âˆˆ [0, 1]
        - global_cond: (B, obs_dim) è§‚æµ‹æ¡ä»¶
        
    è¾“å‡º:
        - velocity: (B, pred_horizon, action_dim) é¢„æµ‹çš„é€Ÿåº¦åœº
    """
```

**æ¨ç†è¿‡ç¨‹**ï¼ˆODE ç§¯åˆ†ï¼‰ï¼š
```python
x = torch.randn(B, pred_horizon, action_dim)  # ä»å™ªå£°å¼€å§‹
dt = 1.0 / num_inference_steps

for i in range(num_inference_steps):
    t = torch.full((B,), i * dt)
    v = velocity_net(x, t, dt, obs_cond)
    x = x + dt * v  # Euler ç§¯åˆ†
    
return x  # ç”Ÿæˆçš„åŠ¨ä½œåºåˆ—
```

### 2. EnsembleQNetwork

ä¸ºäº†è§£å†³ Q ä¼°è®¡çš„æ–¹å·®é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ **Ensemble Q-Network**ï¼š

```python
class EnsembleQNetwork(nn.Module):
    """
    å¯é…ç½®æ•°é‡çš„ Q ç½‘ç»œé›†æˆã€‚
    
    å‚æ•°:
        num_qs: Q ç½‘ç»œæ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰
        num_min_qs: å­é‡‡æ ·å–æœ€å°çš„ç½‘ç»œæ•°ï¼ˆé»˜è®¤ 2ï¼‰
        
    å…³é”®æ–¹æ³•:
        forward(actions, obs) â†’ (num_qs, B, 1)  # æ‰€æœ‰ Q å€¼
        get_min_q(actions, obs) â†’ (B, 1)        # ä¿å®ˆä¼°è®¡
        get_mean_q(actions, obs) â†’ (B, 1)       # å‡å€¼ä¼°è®¡
    """
```

**RLPD é£æ ¼çš„ä¿å®ˆ Q ä¼°è®¡**ï¼š
```python
def get_min_q(self, action_seq, obs_cond, random_subset=True):
    q_all = self.forward(action_seq, obs_cond)  # (num_qs, B, 1)
    
    if random_subset and self.num_min_qs < self.num_qs:
        # éšæœºé€‰æ‹© num_min_qs ä¸ªç½‘ç»œ
        indices = torch.randperm(self.num_qs)[:self.num_min_qs]
        q_subset = q_all[indices]
    else:
        q_subset = q_all
    
    return q_subset.min(dim=0).values  # å–æœ€å°å€¼
```

**ä¸ºä»€ä¹ˆç”¨ Ensembleï¼Ÿ**
- å‡å°‘ Q å‡½æ•°çš„è¿‡ä¼°è®¡
- éšæœºå­é‡‡æ ·å¢åŠ è®­ç»ƒå¤šæ ·æ€§
- ä¸ RLPD åŸè®ºæ–‡æ¨èçš„ `num_qs=10, num_min_qs=2` ä¸€è‡´

### 3. LayerNorm ç¨³å®šæ€§

Q ç½‘ç»œä¸­ä½¿ç”¨ **LayerNorm** è€Œé BatchNormï¼š

```python
# EnsembleQNetwork å†…éƒ¨ç»“æ„
for hidden_dim in hidden_dims:
    q_layers.extend([
        nn.Linear(in_dim, hidden_dim),
        nn.LayerNorm(hidden_dim),  # å…³é”®ï¼šç¨³å®šè®­ç»ƒ
        nn.Mish(),
    ])
```

**LayerNorm çš„ä¼˜åŠ¿**ï¼š
- ä¸ä¾èµ– batch ç»Ÿè®¡é‡ï¼Œå° batch size ä¸‹ç¨³å®š
- ä¸ Transformer æ¶æ„ä¸€è‡´ï¼ˆUNet å†…éƒ¨ä¹Ÿç”¨ GroupNormï¼‰
- é˜²æ­¢ Q å€¼çˆ†ç‚¸

### 4. Advantage Weighting

æ ¸å¿ƒçš„ä¼˜åŠ¿åŠ æƒæœºåˆ¶ï¼ˆAWAC-styleï¼‰ï¼š

```python
def _compute_advantage_weights(self, actions_for_q, obs_cond):
    with torch.no_grad():
        # è·å–ä¿å®ˆ Q ä¼°è®¡
        q_data = self.critic.get_min_q(actions_for_q, obs_cond, random_subset=True)
        
        # è®¡ç®—ä¼˜åŠ¿: A(s,a) = Q(s,a) - V(s)
        baseline = q_data.mean()  # ç”¨ batch å‡å€¼è¿‘ä¼¼ V(s)
        advantage = q_data - baseline
        
        # æŒ‡æ•°åŠ æƒ + è£å‰ª
        weights = torch.clamp(
            torch.exp(self.beta * advantage),
            max=self.weight_clip
        )
        weights = weights / weights.mean()  # å½’ä¸€åŒ–
        
    return weights
```

**å…³é”®å‚æ•°**ï¼š
- `beta`: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ Q å·®å¼‚çš„æ•æ„Ÿåº¦ï¼ˆæ¨è 10~100ï¼‰
- `weight_clip`: é˜²æ­¢æƒé‡è¿‡å¤§ï¼ˆæ¨è 100~200ï¼‰

---

## ğŸ”„ SMDP Action Chunking

Action Chunking å°†æ ‡å‡† MDP è½¬æ¢ä¸º **Semi-MDP (SMDP)**ï¼š

```
æ ‡å‡† MDP: s_t â†’ a_t â†’ r_t â†’ s_{t+1}

Action Chunking SMDP:
s_t â†’ [a_t, a_{t+1}, ..., a_{t+H-1}] â†’ R_cum â†’ s_{t+H}
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              act_horizon æ­¥
```

**SMDP Bellman æ–¹ç¨‹**ï¼š
```python
# ç´¯ç§¯å¥–åŠ±å’ŒæŠ˜æ‰£å› å­
R_cum = Î£_{i=0}^{H-1} Î³^i * r_{t+i}
Î³_H = Î³^H if not done else 0

# TD Target
target_Q = R_cum + Î³_H * min_q(s_{t+H}, Ï€(s_{t+H}))
```

**å…³é”®ç»´åº¦**ï¼š
```python
obs_horizon = 2      # è§‚æµ‹å†å²é•¿åº¦
pred_horizon = 16    # é¢„æµ‹çš„åŠ¨ä½œåºåˆ—é•¿åº¦
act_horizon = 8      # å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°ï¼ˆç”¨äº Q-learningï¼‰
```

---

## ğŸ›¡ï¸ Policy-Critic æ•°æ®åˆ†ç¦»

åœ¨ Online RL é˜¶æ®µï¼Œä¸ºäº†é¿å…å¤±è´¥æ ·æœ¬æ±¡æŸ“ Policy è®­ç»ƒï¼š

```python
# æ•°æ®åˆ†ç¦»ç­–ç•¥
if self.filter_policy_data and is_demo is not None:
    with torch.no_grad():
        q_values = self.critic.get_min_q(actions_for_q, obs_cond)
        baseline = q_values.mean()
        advantage = q_values - baseline
    
    # ä¿ç•™: demo æ ·æœ¬ + é«˜ advantage çš„åœ¨çº¿æ ·æœ¬
    keep_mask = is_demo | (advantage.squeeze() > self.advantage_threshold)
    
    if keep_mask.sum() > 0:
        # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®è®­ç»ƒ Policy
        obs_features_filtered = obs_features[keep_mask]
        actions_filtered = actions[keep_mask]
        ...
```

**æ•ˆæœ**ï¼š
- **Critic**: ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œå­¦ä¹ å®Œæ•´çš„ Q å‡½æ•°
- **Policy**: åªä½¿ç”¨é«˜è´¨é‡æ•°æ®ï¼ˆdemo + æˆåŠŸæ¢ç´¢ï¼‰

---

## ğŸ“Š å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `beta` | 100.0 | Advantage æ¸©åº¦ï¼Œè¶Šå¤§æƒé‡å·®å¼‚è¶Šæ˜æ˜¾ |
| `weight_clip` | 200.0 | é˜²æ­¢æƒé‡çˆ†ç‚¸ |
| `num_qs` | 10 | Ensemble Q ç½‘ç»œæ•°é‡ |
| `num_min_qs` | 2 | ä¿å®ˆä¼°è®¡çš„å­é‡‡æ ·æ•° |
| `gamma` | 0.9 | æŠ˜æ‰£å› å­ï¼ˆaction chunking ä¸‹ç”¨è¾ƒå°å€¼ï¼‰ |
| `tau` | 0.005 | Target network è½¯æ›´æ–°ç³»æ•° |
| `online_ratio` | 0.5 | åœ¨çº¿/ç¦»çº¿æ•°æ®æ··åˆæ¯”ä¾‹ |
| `utd_ratio` | 8 | Update-to-Data ratio |
| `lr_actor` | 3e-4 | Policy å­¦ä¹ ç‡ |
| `lr_critic` | 3e-4 | Critic å­¦ä¹ ç‡ |

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### Stage 2: Offline RL é¢„è®­ç»ƒ
```bash
python train_offline_rl.py \
    --algorithm aw_shortcut_flow \
    --env_id LiftPegUpright-v1 \
    --obs_mode rgb \
    --demo_path demos/LiftPegUpright-v1.h5 \
    --use_ensemble_q \
    --num_qs 10 \
    --num_min_qs 2 \
    --beta 100.0 \
    --weight_clip 200.0 \
    --total_iters 100000
```

### Stage 3: Online RL å¾®è°ƒ
```bash
python train_rlpd_online.py \
    --algorithm awsc \
    --env_id LiftPegUpright-v1 \
    --obs_mode rgb \
    --pretrained_path runs/awsc-offline/checkpoints/best.pt \
    --load_critic \
    --demo_path demos/LiftPegUpright-v1.h5 \
    --num_qs 10 \
    --num_min_qs 2 \
    --gamma 0.9 \
    --beta 100.0 \
    --utd_ratio 8 \
    --filter_policy_data \
    --total_timesteps 500000
```


